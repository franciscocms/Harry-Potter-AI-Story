{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Harry Potter - TextGeneration.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO0NItVimnIs",
        "colab_type": "text"
      },
      "source": [
        "# What is an RNN?\n",
        "\n",
        "A Recurrent Neural Network is different from the other neural networks as it has a memory which stores information of all the layers it has processed so far and computes the next layer on the basis of this memory.\n",
        "\n",
        "**GRU vs LSTM**\n",
        "\n",
        "Both of these are great for text generation but GRUs are a newer concept…and there isn’t actually a way to determine which one is better in general. Tuning your hyper-parameters well is what will improve your model performance more than choosing a good architecture.²\n",
        "However, if the amount of data is not a problem, LSTMs perform better. If you have less data, GRUs have fewer parameters so they train faster and work well to generalize the lesser data.\n",
        "\n",
        "**Why character-based?**\n",
        "\n",
        "When working with large datasets like this, the complete number of unique words in a corpus is much higher than the number of unique characters. A large dataset will have many many unique words, and when we assign one-hot encodings to such large matrices we’re likely to run into memory issues. Our labels alone can take up storage of terabytes of RAM.\n",
        "So, the same principles which you use to predict words can be applied here, but now you’ll be working with much smaller vocabulary size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W01lnPw_mKoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVFHpxJlnwmG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "1d947d7e-8536-4811-96e7-5384c9814752"
      },
      "source": [
        "files = ['/content/sample_data/1SorcerersStone.txt',\n",
        "         '/content/sample_data/2ChamberofSecrets.txt',\n",
        "         '/content/sample_data/3ThePrisonerOfAzkaban.txt',\n",
        "         '/content/sample_data/4TheGobletOfFire.txt',\n",
        "         '/content/sample_data/5OrderofthePhoenix.txt',\n",
        "         '/content/sample_data/6TheHalfBloodPrince.txt',\n",
        "         '/content/sample_data/7DeathlyHollows.txt']\n",
        "\n",
        "with open('harrypotter.txt', 'w') as outfile:\n",
        "  for file in files:\n",
        "    with open(file) as infile:\n",
        "      outfile.write(infile.read())\n",
        "\n",
        "text = open('harrypotter.txt').read()       \n",
        "print(text[:300])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rry Potter and the Sorcerer's Stone \n",
            "\n",
            "CHAPTER ONE \n",
            "\n",
            "THE BOY WHO LIVED \n",
            "\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they ju\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaHt3suIpLuf",
        "colab_type": "text"
      },
      "source": [
        "# Processing the data\n",
        "\n",
        "We map all the unique character strings in *vocab* to numbers by making two look-up tables:\n",
        "\n",
        "* mapping the characters to numbers (**char2index**)\n",
        "* mapping the numbers back to the characters (**index2char**)\n",
        "\n",
        "Then convert our text to numbers.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj2s7lmRpcze",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5e5a249c-eb64-4a4f-aba9-eecfb01db650"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print(vocab)\n",
        "char2index = {u:i for i, u in enumerate(vocab)}\n",
        "#print(char2index)\n",
        "index2char = np.array(vocab)\n",
        "text_as_int = np.array([char2index[c] for c in text]) #array with mapped elements according to vocab \n",
        "\n",
        "#how it looks:\n",
        "print ('{} -- characters mapped to int -- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\t', '\\n', '\\x1f', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '~', '\\x90', '\\x92', '¦', '«', '\\xad', '»', 'é', 'ü', '–', '‘', '’', '“', '•']\n",
            "'rry Potter an' -- characters mapped to int -- > [81 81 88  3 47 78 83 83 68 81  3 64 77]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-1BrG5yqeSJ",
        "colab_type": "text"
      },
      "source": [
        "Each input sequence for our model will contain *seq_length* number of characters from the text, and its corresponding target sequence will be of the same length with all characters shifted one place to the right. So we break the text into chunks of *seq_length + 1*.\n",
        "\n",
        "**tf.data.Dataset.from_tensor_slices** converts the text vector into a stream of character indices and the **batch** method lets us group these characters into batches of the required length.\n",
        "\n",
        "By using the **map** method to apply a simple function to each batch, we create our inputs and targets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQa4_ha0qTvN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "444bace6-8380-4fc6-8436-8b74583575c7"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length+1)\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True) \n",
        "\n",
        "def split_input_target(data):\n",
        "  input_text = data[:-1]\n",
        "  target_text = data[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "#print(split_input_target('ola'))\n",
        "dataset = sequences.map(split_input_target) #applies the batch method to all entries\n",
        "print(dataset)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3tvWXK0r9RI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e4fdb66f-4660-4e02-aec9-9080127204f9"
      },
      "source": [
        "batch_size = 64\n",
        "buffer_size = 10000\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "print(dataset)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RczzSmusBbL",
        "colab_type": "text"
      },
      "source": [
        "# Building the Model\n",
        "\n",
        "Given all the characters computed until this moment, what will the next character be? This is what we will be training our RNN model to predict.\n",
        "\n",
        "I have used **tf.keras.Sequential** to define the model since all the layers in it only have a single input and produce a single output. The different layers used are:\n",
        "\n",
        "* **tf.keras.layers.Embedding**: This is the input layer. An embedding is used to map all the unique characters to vectors in multi-dimensional space, having embedding_dim dimensions.\n",
        "\n",
        "* **tf.keras.layers.GRU**: A type of RNN with rnn_units number of units.(You can also use an LSTM layer here to see what works best for your data)\n",
        "\n",
        "* **tf.keras.layers.Dense**: This is the output layer, with vocab_size outputs.\n",
        "\n",
        "It is also useful to define all the hyper-parameters separately so that it’s easier for you to change them later without editing the model definition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io7FoMTFsc6L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "7cc4d5b1-fd3f-470e-aa7c-a650d632320e"
      },
      "source": [
        "def build_model(batch_size, vocab_size, embedding_dim, rnn_units1, rnn_units2):\n",
        "  model = tf.keras.Sequential([\n",
        "                               tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape = [batch_size, None]),\n",
        "                               tf.keras.layers.LSTM(rnn_units1, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "                               #tf.keras.layers.LSTM(rnn_units2, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "                               tf.keras.layers.Dense(vocab_size, activation='sigmoid')\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units1 = 1024\n",
        "rnn_units2 = 150\n",
        "\n",
        "model = build_model(batch_size, vocab_size, embedding_dim, rnn_units1, rnn_units2)\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           27136     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 106)           108650    \n",
            "=================================================================\n",
            "Total params: 5,382,762\n",
            "Trainable params: 5,382,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoWLPYqitZKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, probs):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels,\n",
        "         probs, from_logits=False)\n",
        "  \n",
        "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rve9XU7ptjUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "   filepath=checkpoint_prefix, save_weights_only=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_u2OQ5Jts8L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "83ec8df7-eaa1-4309-8f75-b0580a3364a4"
      },
      "source": [
        "EPOCHS= 25\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "latest_check = tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "967/967 [==============================] - 50s 52ms/step - loss: 2.2737 - accuracy: 0.3591\n",
            "Epoch 2/25\n",
            "967/967 [==============================] - 51s 53ms/step - loss: 1.7234 - accuracy: 0.4989\n",
            "Epoch 3/25\n",
            "967/967 [==============================] - 51s 52ms/step - loss: 1.6091 - accuracy: 0.5292\n",
            "Epoch 4/25\n",
            "967/967 [==============================] - 51s 53ms/step - loss: 1.5455 - accuracy: 0.5464\n",
            "Epoch 5/25\n",
            "967/967 [==============================] - 50s 52ms/step - loss: 1.5031 - accuracy: 0.5581\n",
            "Epoch 6/25\n",
            "967/967 [==============================] - 51s 53ms/step - loss: 1.4714 - accuracy: 0.5662\n",
            "Epoch 7/25\n",
            "967/967 [==============================] - 51s 53ms/step - loss: 1.4458 - accuracy: 0.5729\n",
            "Epoch 8/25\n",
            "967/967 [==============================] - 51s 53ms/step - loss: 1.4253 - accuracy: 0.5781\n",
            "Epoch 9/25\n",
            "967/967 [==============================] - 52s 53ms/step - loss: 1.4094 - accuracy: 0.5824\n",
            "Epoch 10/25\n",
            "967/967 [==============================] - 51s 53ms/step - loss: 1.3966 - accuracy: 0.5859\n",
            "Epoch 11/25\n",
            "967/967 [==============================] - 52s 54ms/step - loss: 1.3850 - accuracy: 0.5889\n",
            "Epoch 12/25\n",
            "967/967 [==============================] - 51s 53ms/step - loss: 1.3756 - accuracy: 0.5915\n",
            "Epoch 13/25\n",
            "967/967 [==============================] - 51s 53ms/step - loss: 1.3669 - accuracy: 0.5939\n",
            "Epoch 14/25\n",
            "967/967 [==============================] - 51s 53ms/step - loss: 1.3597 - accuracy: 0.5958\n",
            "Epoch 15/25\n",
            "967/967 [==============================] - 52s 53ms/step - loss: 1.3531 - accuracy: 0.5976\n",
            "Epoch 16/25\n",
            "967/967 [==============================] - 51s 53ms/step - loss: 1.3468 - accuracy: 0.5993\n",
            "Epoch 17/25\n",
            "967/967 [==============================] - 52s 53ms/step - loss: 1.3412 - accuracy: 0.6009\n",
            "Epoch 18/25\n",
            "967/967 [==============================] - 52s 53ms/step - loss: 1.3364 - accuracy: 0.6022\n",
            "Epoch 19/25\n",
            "967/967 [==============================] - 52s 54ms/step - loss: 1.3318 - accuracy: 0.6034\n",
            "Epoch 20/25\n",
            "967/967 [==============================] - 52s 54ms/step - loss: 1.3275 - accuracy: 0.6044\n",
            "Epoch 21/25\n",
            "967/967 [==============================] - 52s 54ms/step - loss: 1.3238 - accuracy: 0.6056\n",
            "Epoch 22/25\n",
            "967/967 [==============================] - 52s 54ms/step - loss: 1.3202 - accuracy: 0.6065\n",
            "Epoch 23/25\n",
            "967/967 [==============================] - 52s 54ms/step - loss: 1.3167 - accuracy: 0.6073\n",
            "Epoch 24/25\n",
            "967/967 [==============================] - 52s 54ms/step - loss: 1.3135 - accuracy: 0.6082\n",
            "Epoch 25/25\n",
            "967/967 [==============================] - 52s 54ms/step - loss: 1.3108 - accuracy: 0.6092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDzDM-J0TOy3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "78d1666c-fa48-46d8-fb3a-1361b521022d"
      },
      "source": [
        "# now eith 1024 rnn units\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units1 = 1024\n",
        "rnn_units2 = 150\n",
        "\n",
        "model = build_model(batch_size, vocab_size, embedding_dim, rnn_units1, rnn_units2)\n",
        "model.summary()\n",
        "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "   filepath=checkpoint_prefix, save_weights_only=True)\n",
        "\n",
        "EPOCHS= 25\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "latest_check = tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (64, None, 256)           27136     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (64, None, 106)           108650    \n",
            "=================================================================\n",
            "Total params: 5,382,762\n",
            "Trainable params: 5,382,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "967/967 [==============================] - 168s 174ms/step - loss: 2.2001 - accuracy: 0.3507\n",
            "Epoch 2/25\n",
            "967/967 [==============================] - 168s 174ms/step - loss: 1.5476 - accuracy: 0.5448\n",
            "Epoch 3/25\n",
            "967/967 [==============================] - 168s 173ms/step - loss: 1.4243 - accuracy: 0.5776\n",
            "Epoch 4/25\n",
            "967/967 [==============================] - 167s 173ms/step - loss: 1.3631 - accuracy: 0.5939\n",
            "Epoch 5/25\n",
            "967/967 [==============================] - 168s 174ms/step - loss: 1.3240 - accuracy: 0.6041\n",
            "Epoch 6/25\n",
            "967/967 [==============================] - 168s 174ms/step - loss: 1.2958 - accuracy: 0.6113\n",
            "Epoch 7/25\n",
            "967/967 [==============================] - 168s 174ms/step - loss: 1.2740 - accuracy: 0.6170\n",
            "Epoch 8/25\n",
            "967/967 [==============================] - 168s 174ms/step - loss: 1.2568 - accuracy: 0.6216\n",
            "Epoch 9/25\n",
            "967/967 [==============================] - 167s 173ms/step - loss: 1.2430 - accuracy: 0.6251\n",
            "Epoch 10/25\n",
            "967/967 [==============================] - 167s 173ms/step - loss: 1.2305 - accuracy: 0.6285\n",
            "Epoch 11/25\n",
            "967/967 [==============================] - 168s 174ms/step - loss: 1.2202 - accuracy: 0.6312\n",
            "Epoch 12/25\n",
            "967/967 [==============================] - 168s 173ms/step - loss: 1.2111 - accuracy: 0.6336\n",
            "Epoch 13/25\n",
            "967/967 [==============================] - 167s 173ms/step - loss: 1.2028 - accuracy: 0.6360\n",
            "Epoch 14/25\n",
            "967/967 [==============================] - 168s 174ms/step - loss: 1.1954 - accuracy: 0.6379\n",
            "Epoch 15/25\n",
            "967/967 [==============================] - 169s 174ms/step - loss: 1.1887 - accuracy: 0.6396\n",
            "Epoch 16/25\n",
            "967/967 [==============================] - 169s 175ms/step - loss: 1.1826 - accuracy: 0.6414\n",
            "Epoch 17/25\n",
            "967/967 [==============================] - 168s 174ms/step - loss: 1.1766 - accuracy: 0.6431\n",
            "Epoch 18/25\n",
            "967/967 [==============================] - 168s 174ms/step - loss: 1.1713 - accuracy: 0.6444\n",
            "Epoch 19/25\n",
            "967/967 [==============================] - 168s 174ms/step - loss: 1.1664 - accuracy: 0.6460\n",
            "Epoch 20/25\n",
            "967/967 [==============================] - 168s 174ms/step - loss: 1.1617 - accuracy: 0.6472\n",
            "Epoch 21/25\n",
            "967/967 [==============================] - 168s 174ms/step - loss: 1.1571 - accuracy: 0.6486\n",
            "Epoch 22/25\n",
            "967/967 [==============================] - 168s 174ms/step - loss: 1.1529 - accuracy: 0.6495\n",
            "Epoch 23/25\n",
            "967/967 [==============================] - 168s 174ms/step - loss: 1.1490 - accuracy: 0.6508\n",
            "Epoch 24/25\n",
            "967/967 [==============================] - 167s 173ms/step - loss: 1.1455 - accuracy: 0.6517\n",
            "Epoch 25/25\n",
            "967/967 [==============================] - 167s 172ms/step - loss: 1.1417 - accuracy: 0.6527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLxG_wS8KezG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "60ef7e02-0703-4f64-f812-eb166fb59f76"
      },
      "source": [
        "def build_model(batch_size, vocab_size, embedding_dim, rnn_units1, rnn_units2):\n",
        "  model = tf.keras.Sequential([\n",
        "                               tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape = [batch_size, None]),\n",
        "                               tf.keras.layers.LSTM(rnn_units1, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "                               #tf.keras.layers.LSTM(rnn_units2, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "                               tf.keras.layers.Dense(vocab_size, activation='sigmoid')\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "def loss(labels, probs):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels,\n",
        "         probs, from_logits=False)\n",
        "\n",
        "# now eith 1024 rnn units\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "rnn_units1 = 1024\n",
        "rnn_units2 = 150\n",
        "\n",
        "model = build_model(batch_size, vocab_size, embedding_dim, rnn_units1, rnn_units2)\n",
        "model.summary()\n",
        "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "   filepath=checkpoint_prefix, save_weights_only=True)\n",
        "\n",
        "EPOCHS= 25\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "latest_check = tf.train.latest_checkpoint(checkpoint_dir)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (64, None, 300)           31800     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (64, None, 1024)          5427200   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (64, None, 106)           108650    \n",
            "=================================================================\n",
            "Total params: 5,567,650\n",
            "Trainable params: 5,567,650\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "967/967 [==============================] - 179s 185ms/step - loss: 2.1718 - accuracy: 0.3593\n",
            "Epoch 2/25\n",
            "967/967 [==============================] - 178s 184ms/step - loss: 1.5459 - accuracy: 0.5443\n",
            "Epoch 3/25\n",
            "967/967 [==============================] - 178s 184ms/step - loss: 1.4214 - accuracy: 0.5777\n",
            "Epoch 4/25\n",
            "967/967 [==============================] - 175s 181ms/step - loss: 1.3602 - accuracy: 0.5938\n",
            "Epoch 5/25\n",
            "967/967 [==============================] - 174s 180ms/step - loss: 1.3207 - accuracy: 0.6043\n",
            "Epoch 6/25\n",
            "967/967 [==============================] - 175s 181ms/step - loss: 1.2935 - accuracy: 0.6114\n",
            "Epoch 7/25\n",
            "967/967 [==============================] - 175s 181ms/step - loss: 1.2721 - accuracy: 0.6173\n",
            "Epoch 8/25\n",
            "967/967 [==============================] - 175s 181ms/step - loss: 1.2550 - accuracy: 0.6216\n",
            "Epoch 9/25\n",
            "967/967 [==============================] - 175s 181ms/step - loss: 1.2405 - accuracy: 0.6257\n",
            "Epoch 10/25\n",
            "967/967 [==============================] - 174s 180ms/step - loss: 1.2289 - accuracy: 0.6289\n",
            "Epoch 11/25\n",
            "967/967 [==============================] - 174s 180ms/step - loss: 1.2188 - accuracy: 0.6316\n",
            "Epoch 12/25\n",
            "967/967 [==============================] - 174s 180ms/step - loss: 1.2095 - accuracy: 0.6341\n",
            "Epoch 13/25\n",
            "967/967 [==============================] - 174s 180ms/step - loss: 1.2011 - accuracy: 0.6363\n",
            "Epoch 14/25\n",
            "967/967 [==============================] - 175s 181ms/step - loss: 1.1936 - accuracy: 0.6384\n",
            "Epoch 15/25\n",
            "967/967 [==============================] - 175s 181ms/step - loss: 1.1875 - accuracy: 0.6401\n",
            "Epoch 16/25\n",
            "967/967 [==============================] - 175s 181ms/step - loss: 1.1808 - accuracy: 0.6419\n",
            "Epoch 17/25\n",
            "967/967 [==============================] - 175s 181ms/step - loss: 1.1750 - accuracy: 0.6435\n",
            "Epoch 18/25\n",
            "967/967 [==============================] - 175s 181ms/step - loss: 1.1701 - accuracy: 0.6448\n",
            "Epoch 19/25\n",
            "967/967 [==============================] - 174s 180ms/step - loss: 1.1653 - accuracy: 0.6463\n",
            "Epoch 20/25\n",
            "967/967 [==============================] - 174s 180ms/step - loss: 1.1605 - accuracy: 0.6475\n",
            "Epoch 21/25\n",
            "967/967 [==============================] - 174s 180ms/step - loss: 1.1562 - accuracy: 0.6487\n",
            "Epoch 22/25\n",
            "967/967 [==============================] - 174s 180ms/step - loss: 1.1522 - accuracy: 0.6497\n",
            "Epoch 23/25\n",
            "967/967 [==============================] - 174s 180ms/step - loss: 1.1482 - accuracy: 0.6509\n",
            "Epoch 24/25\n",
            "967/967 [==============================] - 174s 180ms/step - loss: 1.1445 - accuracy: 0.6520\n",
            "Epoch 25/25\n",
            "967/967 [==============================] - 173s 179ms/step - loss: 1.1410 - accuracy: 0.6530\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH1TV4FyV9Uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  \n",
        "  num_generate = 1000 #anything\n",
        "\n",
        "  input_eval = [char2index[s] for s in start_string] \n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  text_generated = []\n",
        "\n",
        "  scaling = 0.5 #kept at a lower value here\n",
        "\n",
        "  #batch_size = 1 now!!\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    #remove batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / scaling\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples = 1)[0, 0].numpy()\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    text_generated.append(index2char[predicted_id])\n",
        "  \n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwsuxRNDXEGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(1, vocab_size, embedding_dim, rnn_units1, rnn_units2)\n",
        "model.load_weights(latest_check)\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()\n",
        "\n",
        "print(generate_text(model, start_string=u'Severus Snape'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ5YVDIbXElF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(generate_text(model, start_string=u'Voldemort died'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a34s26SJXNfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(generate_text(model, start_string=u'Harry and Ron '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln7y2UY3XQi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(generate_text(model, start_string=u'Dumbledore said to Harry'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}